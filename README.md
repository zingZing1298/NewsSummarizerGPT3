The goal of this project is to investigate papers on natural language processing and replicate the
techniques and findings presented in the papers. Specifically, we chose to focus on the T5:
Text-to-Text Transformer paper and replicate its results. T5 is an advanced language model
developed by Google that employs the Transformer architecture, which is a deep neural network
architecture commonly used in NLP tasks. T5 was trained on a large dataset that included
various NLP tasks, such as text classification, language translation, question answering, and
summarization.
We also delved into text summarization, which has practical applications in fields such as
medicine and research. Summarization is useful for quickly finding important and relevant
information from lengthy documents.
To perform text summarization, we fine-tuned T5 on a news summarization dataset that included
article headlines and corresponding descriptions. Intrigued by the results, we compared the
summarization abilities of several state-of-the-art models, including BART, Pegasus, T5-base
and GPT2 models, without fine-tuning them, to establish a ranking among them.
